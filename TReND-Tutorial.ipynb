{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e53337a-28a4-4b19-b005-0292b324d389",
   "metadata": {},
   "source": [
    "# Tutorial for TReND: **T**ransformer-derived features and **Re**gularized **N**MF for of neonatal functional networks **D**elineation \n",
    "\n",
    "This tutorial walks you through reproducing the results from our study, starting from the post-processing of rs-fMRI data. All visualizations are carried out using the Workbench view. Detailed steps include compute resource requirements and estimated time for each stage of the process.\n",
    "\n",
    "All rights reserved by LINC at Penn and CHOP.\n",
    "\n",
    "### **Author of the Code**  \n",
    "**Sovesh Mohapatra**  \n",
    "PhD Candidate, University of Pennsylvania  \n",
    "\n",
    "---\n",
    "\n",
    "### **Advisors**  \n",
    "**Dr. Hao Huang** and **Dr. Minhui Ouyang**   \n",
    "University of Pennsylvania and Children Hospital of Philadelphia \n",
    "\n",
    "---\n",
    "\n",
    "### **Lab Affiliation**  \n",
    "**LINBC: Laboratory for Intelligent Neuroimaging and Brain Connectivity**  \n",
    "University of Pennsylvania and Children Hospital of Philadelphia\n",
    "\n",
    "---\n",
    "\n",
    "### **Contact Information**\n",
    "\n",
    "- **Sovesh Mohapatra**: [soveshm@seas.upenn.edu](mailto:soveshm@seas.upenn.edu)  \n",
    "- **Dr. Minhui Ouyang**: [ouyangm@chop.edu](mailto:ouyangm@chop.edu)  \n",
    "- **Dr. Hao Huang**: [huangh6@chop.edu](mailto:huangh6@chop.edu)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189aab3d-5c61-4523-bf16-44f693c33a3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Total expected compute resources and time required to complete the process from scratch\n",
    "\n",
    "- **GPUs**: 4 NVIDIA A100s with 80GB each\n",
    "- **CPUs**: 60 cores, each with 256GB of RAM\n",
    "- **Expected Time**: Approximately 114hrs (~4.75 days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f40a7b-2e5d-4e18-85ea-35992a675a9a",
   "metadata": {},
   "source": [
    "## Step 1: Feature extraction from the processed BOLD signals using transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76dab8d-0ecf-4962-90a5-1c26d2bcd322",
   "metadata": {},
   "source": [
    "#### Compute resources and expected time\n",
    "\n",
    "- **GPUs**: 4 NVIDIA A100s with 80GB each\n",
    "- **CPUs**: 4 cores, each with 256GB of RAM\n",
    "- **Expected Time**: Approximately 72 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7d0e7-d5b7-49f1-a222-43ff3d2d53c8",
   "metadata": {},
   "source": [
    "##### **Optional**: If you want you use the trained model. Please run Step 1a through 1f and 1g to generate the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a05169-050c-4089-9bfd-956911e7fa09",
   "metadata": {},
   "source": [
    "#### **Step 1a.** Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97960484-44bc-4f2f-9ca7-732953e3b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BOLDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_folder, file_list, train=True):\n",
    "        self.data_folder = data_folder\n",
    "        self.file_list = file_list\n",
    "        self.train = train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = os.path.join(self.data_folder, self.file_list[index])\n",
    "        data = np.load(file_path)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "# Point to the data folder containing the npy files\n",
    "data_folder = 'FBP/data'\n",
    "# Get all .npy file names\n",
    "data_files = [f for f in os.listdir(data_folder) if f.endswith('.npy')]\n",
    "\n",
    "# Split into 75% train, 25% test\n",
    "train_files, test_files = train_test_split(data_files, test_size=0.25, random_state=108)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BOLDDataset(data_folder, train_files, train=True)\n",
    "test_dataset = BOLDDataset(data_folder, test_files, train=False)\n",
    "final_dataset = BOLDDataset(data_folder, data_files)\n",
    "\n",
    "# Data Loaders with batch size and number of workers\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "final_loader = torch.utils.data.DataLoader(final_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c305b964-3170-466d-827f-6255e3f8a478",
   "metadata": {},
   "source": [
    "#### **Step 1b.** Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420e014-f0c4-496d-b5dd-5161f29a6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ConfidenceModule(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        confidence_scores = self.mlp(x)\n",
    "        return confidence_scores.squeeze(-1)  \n",
    "\n",
    "class ConfidenceAdaptiveMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % nhead == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.d_k = d_model // nhead\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.confidence_module = ConfidenceModule(d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        batch_size, seq_len, d_model = query.size()\n",
    "        \n",
    "        query_transposed = query.transpose(0, 1)\n",
    "        confidence_scores = self.confidence_module(query_transposed)  # (seq_len, batch_size)\n",
    "        \n",
    "        eps = 1e-8\n",
    "        log_confidence = torch.log(confidence_scores + eps)  # (seq_len, batch_size)\n",
    "        \n",
    "        confidence_mask = log_confidence.unsqueeze(2) + log_confidence.unsqueeze(1)  # (seq_len, batch_size, seq_len)\n",
    "        confidence_mask = confidence_mask.transpose(0, 1)  # (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        Q = self.w_q(query).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attention_output = self.scaled_dot_product_attention(Q, K, V, confidence_mask, attn_mask, key_padding_mask)\n",
    "        \n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        return output, None  \n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, confidence_mask, attn_mask=None, key_padding_mask=None):\n",
    "        batch_size, nhead, seq_len, d_k = Q.size()\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        confidence_mask = confidence_mask.unsqueeze(1).expand(batch_size, nhead, seq_len, seq_len)\n",
    "        scores = scores + confidence_mask\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            scores += attn_mask\n",
    "        \n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(key_padding_mask, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=1600, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = ConfidenceAdaptiveMultiHeadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=1600, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = ConfidenceAdaptiveMultiHeadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = ConfidenceAdaptiveMultiHeadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2, _ = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, d_model, nhead):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        return self.norm(output)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, d_model, nhead):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.linear = nn.Linear(d_model, 1600)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        output = tgt\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = self.norm(output)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02990356-f548-4cfc-8645-ecd8a33f9ca1",
   "metadata": {},
   "source": [
    "#### **Step 1c.** Model and weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ee7e0-6dee-4d95-a36a-9fd308cba28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512  \n",
    "nhead = 8      \n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "transformer_encoder_layer = TransformerEncoderLayer(d_model, nhead)\n",
    "transformer_decoder_layer = TransformerDecoderLayer(d_model, nhead)\n",
    "\n",
    "transformer_encoder = TransformerEncoder(transformer_encoder_layer, num_encoder_layers, d_model, nhead)\n",
    "transformer_decoder = TransformerDecoder(transformer_decoder_layer, num_decoder_layers, d_model, nhead)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, ConfidenceModule):\n",
    "        for layer in m.mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight, gain=0.5)\n",
    "                if layer.bias is not None:\n",
    "                    if layer == m.mlp[-2]:  \n",
    "                        nn.init.constant_(layer.bias, 0.0)  \n",
    "                    else:\n",
    "                        nn.init.zeros_(layer.bias)\n",
    "\n",
    "transformer_encoder.apply(initialize_weights)\n",
    "transformer_decoder.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84246ba2-bde3-4f8b-9624-52c0ff6a4da0",
   "metadata": {},
   "source": [
    "#### **Step 1d.** Autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465aed45-94d6-46da-b160-cec85ab45522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(1600, d_model)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x_embedded = self.embedding(x) \n",
    "        encoded = self.encoder(x_embedded)\n",
    "        decoded = self.decoder(encoded, encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dcb8f-2b1a-48f7-995e-f94fbc81030b",
   "metadata": {},
   "source": [
    "#### **Step 1e.** Preparation for model training and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0202d1-666d-4dfb-af52-5cf04afc9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TransformerAutoencoder(transformer_encoder, transformer_decoder).to(device)\n",
    "model = model.float()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.mae = nn.L1Loss()\n",
    "        self.alpha = alpha\n",
    "    def forward(self, outputs, targets):\n",
    "        mse_loss = self.mse(outputs, targets)\n",
    "        mae_loss = self.mae(outputs, targets)\n",
    "        return self.alpha * mse_loss + (1 - self.alpha) * mae_loss\n",
    "\n",
    "criterion = CombinedLoss(alpha=0.5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "\n",
    "best_model_path = 'TF_lr1e-5_bs4_512.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8a073-8e52-4ca5-a300-79f29d9b1f7e",
   "metadata": {},
   "source": [
    "#### **Step 1f.** Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f5734-9bae-43f3-970e-a07158ff6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1500\n",
    "best_val_loss = float('inf')\n",
    "start_epoch = 0\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Resuming from epoch {start_epoch} with best val loss {best_val_loss}\")\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data.to(device)\n",
    "        inputs = inputs.float()\n",
    "        \n",
    "        if torch.isnan(inputs).any():\n",
    "            print(\"NaN in inputs!\")\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if torch.isnan(outputs).any():\n",
    "            print(\"NaN in outputs!\")\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(outputs, inputs)\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN in loss!\")\n",
    "            continue\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"NaN in gradients of {name}\")\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"Inf in gradients of {name}\")\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs = data.to(device)\n",
    "            inputs = inputs.float()\n",
    "            \n",
    "            if torch.isnan(inputs).any():\n",
    "                print(\"NaN in validation inputs!\")\n",
    "                continue\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if torch.isnan(outputs).any():\n",
    "                print(\"NaN in validation outputs!\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, inputs)\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(\"NaN in validation loss!\")\n",
    "                continue\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"New Learning Rate: {param_group['lr']}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(\"Saved with better loss.\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, best_model_path)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train loss: {train_loss/len(train_loader.dataset)}, Val Loss: {val_loss/len(test_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e0ffe-8947-4481-8b55-c777e3f4db41",
   "metadata": {},
   "source": [
    "#### **Step 1g.** Generate and save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7739313-14fb-4971-b7d5-8094b6a4a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(dataset, loader, filename):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data.to(device)\n",
    "            inputs = inputs.float()\n",
    "            outputs = best_model.embedding(inputs)  # Get embeddings from the best model\n",
    "            embeddings.append(outputs.cpu().numpy())\n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    np.save(filename, embeddings)\n",
    "\n",
    "\n",
    "save_embeddings(train_dataset, final_loader, \n",
    "                '/TF_lr1e-5_512_emb.npy/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0910c0-4ff3-49bb-a5d7-d6b3319bbdba",
   "metadata": {},
   "source": [
    "#### **Step 2.** Pearson Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da1347-bca3-42cb-b293-d0c09f2c4ed0",
   "metadata": {},
   "source": [
    "#### **Step 3.** Thresholding and Parcellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from joblib import dump, load\n",
    "from itertools import product\n",
    "import gc\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38254c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_matrix_by_label(matrix, labels):\n",
    "    thresholded_matrix = np.zeros_like(matrix)\n",
    "    for row_idx, row_label in enumerate(labels):\n",
    "        thresholded_matrix[row_idx, labels == row_label] = matrix[row_idx, labels == row_label]\n",
    "    return thresholded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_cluster_connectivity(fc_matrix, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    within_conn = []\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        cluster_fc = fc_matrix[np.ix_(indices, indices)]\n",
    "        if len(indices) > 1:\n",
    "            avg_conn = np.mean(cluster_fc[np.triu_indices(len(indices), 1)])\n",
    "            within_conn.append(avg_conn)\n",
    "        else:\n",
    "            within_conn.append(0) \n",
    "    return np.mean(within_conn)\n",
    "\n",
    "def between_cluster_connectivity(fc_matrix, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    between_conn = []\n",
    "    for i, label_i in enumerate(unique_labels):\n",
    "        for label_j in unique_labels[i+1:]:\n",
    "            indices_i = np.where(labels == label_i)[0]\n",
    "            indices_j = np.where(labels == label_j)[0]\n",
    "            cluster_fc = fc_matrix[np.ix_(indices_i, indices_j)]\n",
    "            avg_conn = np.mean(cluster_fc)\n",
    "            between_conn.append(avg_conn)\n",
    "    return np.mean(between_conn)\n",
    "\n",
    "def compute_metrics_for_params(params, data_filename, label_file):\n",
    "\n",
    "    n_components, alpha_W, l1_ratio, n_clusters = params\n",
    "    print(f\"Starting combination: n_components={n_components}, alpha_W={alpha_W}, l1_ratio={l1_ratio}, n_clusters={n_clusters}\", flush=True)\n",
    "    kernel_matrix = load(data_filename, mmap_mode='r')\n",
    "\n",
    "    nmf_model = NMF(n_components=n_components, init='nndsvda', random_state=108,\n",
    "                    l1_ratio=l1_ratio, max_iter=10000, tol=1e-4, solver='cd',\n",
    "                    beta_loss='frobenius', alpha_W=alpha_W, verbose=False)\n",
    "    nmf_features = nmf_model.fit_transform(kernel_matrix)\n",
    "    print('NMF completed.', flush=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=108)\n",
    "    kmeans.fit(nmf_features)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    print('KMeans completed.', flush=True)\n",
    "\n",
    "    cluster_save = f\"/{n_components}_comp_{n_clusters}_cl_{l1_ratio}_{alpha_W}.npy\"\n",
    "    np.save(cluster_save, cluster_labels)\n",
    "\n",
    "    voxel_counts = np.bincount(cluster_labels, minlength=n_clusters)\n",
    "    voxel_count_columns = [f'Voxels_in_Cluster_{i+1}' for i in range(n_clusters)]\n",
    "\n",
    "    try:\n",
    "        average_SI = silhouette_score(nmf_features, cluster_labels)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "        average_SI = None\n",
    "                    \n",
    "    try:\n",
    "        average_DBI = davies_bouldin_score(nmf_features, cluster_labels)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "        average_DBI = None\n",
    "    elbow_method = kmeans.inertia_\n",
    "\n",
    "    within_cluster_conn = within_cluster_connectivity(kernel_matrix, cluster_labels)\n",
    "    between_cluster_conn = between_cluster_connectivity(kernel_matrix, cluster_labels)\n",
    "    connectivity_ratio = within_cluster_conn / between_cluster_conn if between_cluster_conn != 0 else np.inf\n",
    "\n",
    "    new_row_data = {\n",
    "        \"label_file\": label_file,\n",
    "        \"Clustering_Algorithm\": \"NMF\",\n",
    "        \"Helper_Algo\": \"KMeans\",\n",
    "        \"Alpha_W\": alpha_W,\n",
    "        \"L1_ratio\": l1_ratio,\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"n_components\": n_components,\n",
    "        \"Average_SI\": average_SI,\n",
    "        \"Average_DBI\": average_DBI,\n",
    "        \"ElbowMethod\": elbow_method,\n",
    "        \"Within_Cluster_Connectivity\": within_cluster_conn,\n",
    "        \"Between_Cluster_Connectivity\": between_cluster_conn,\n",
    "        \"Connectivity_Ratio\": connectivity_ratio\n",
    "    }\n",
    "\n",
    "    new_row_data.update(dict(zip(voxel_count_columns, voxel_counts)))\n",
    "\n",
    "    print(f\"Finished combination: n_components={n_components}, alpha_W={alpha_W}, \"\n",
    "          f\"l1_ratio={l1_ratio}, n_clusters={n_clusters}\", flush=True)\n",
    "\n",
    "    del nmf_features\n",
    "    del cluster_labels\n",
    "    gc.collect()\n",
    "\n",
    "    return new_row_data\n",
    "\n",
    "def automate_nmf_process(master_results_df, label_file, thresholded_fc_data, n_components_range, n_clusters_range,\n",
    "                         alpha_W_range, l1_ratio_range, temp_folder, batch_size=50):\n",
    "\n",
    "    parameter_combinations = list(product(n_components_range, alpha_W_range, l1_ratio_range, n_clusters_range))\n",
    "    print('Parameter combinations generated.')\n",
    "\n",
    "    data_filename = os.path.join(temp_folder, f'thresholded_fc_data_{label_file}.mmap')\n",
    "    dump(thresholded_fc_data, data_filename)\n",
    "\n",
    "    total_combinations = len(parameter_combinations)\n",
    "    num_batches = (total_combinations + batch_size - 1) // batch_size  # Ceiling division\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_combinations)\n",
    "        batch_params = parameter_combinations[start_idx:end_idx]\n",
    "        print(f\"Processing batch {batch_num+1}/{num_batches}, combinations {start_idx} to {end_idx-1}\", flush=True)\n",
    "\n",
    "        num_jobs = 30  \n",
    "        with ProcessPoolExecutor(max_workers=num_jobs) as executor:\n",
    "            futures = [executor.submit(compute_metrics_for_params, params, data_filename, label_file) for params in batch_params]\n",
    "            results = []\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\", flush=True)\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        master_results_df = pd.concat([master_results_df, results_df], ignore_index=True)\n",
    "\n",
    "        print(f\"Batch {batch_num+1}/{num_batches} completed.\", flush=True)\n",
    "\n",
    "    os.remove(data_filename)\n",
    "\n",
    "    return master_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_df = pd.DataFrame()\n",
    "\n",
    "data = np.load(\"/TF_emb_average_correlation.npy\")\n",
    "print('Data loaded.')\n",
    "\n",
    "label_files_dir = \"Geodesic-Distance\"\n",
    "label_files = [file for file in os.listdir(label_files_dir) if file.endswith('.npy')]\n",
    "\n",
    "alpha_W_range = [1e-3]\n",
    "l1_ratio_range = [0.4]\n",
    "n_components_range = [10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40]\n",
    "n_clusters_range = [14, 38]\n",
    "\n",
    "temp_folder = 'joblib_memmap'\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "for label_file in label_files:\n",
    "    # Load labels\n",
    "    labels = np.load(os.path.join(label_files_dir, label_file))\n",
    "    # Apply thresholding based on labels\n",
    "    thresholded_fc_data = threshold_matrix_by_label(data, labels)\n",
    "    print(f'Data thresholded for label file {label_file}.')\n",
    "    \n",
    "    print('Starting NMF and KMeans processing...')\n",
    "\n",
    "    master_results_df = automate_nmf_process(\n",
    "        master_results_df,\n",
    "        label_file,\n",
    "        thresholded_fc_data,\n",
    "        n_components_range,\n",
    "        n_clusters_range,\n",
    "        alpha_W_range,\n",
    "        l1_ratio_range,\n",
    "        temp_folder,\n",
    "        batch_size=12  \n",
    "    )\n",
    "\n",
    "    del labels\n",
    "    del thresholded_fc_data\n",
    "    gc.collect()\n",
    "    print(f'Processing completed for label file {label_file}.\\n')\n",
    "\n",
    "output_file = \"/Complete_Clustering.xlsx\"\n",
    "master_results_df.to_excel(output_file, index=False)\n",
    "print(f\"Complete pipeline results saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39e0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243445be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
